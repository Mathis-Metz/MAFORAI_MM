{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b3566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Mes Documents\\Pas cours\\Devoir Stage\\Space\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Setup: Ensure src module is importable\n",
    "# =========================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path\n",
    "ROOT = Path(\"..\").resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8f132",
   "metadata": {},
   "source": [
    "# Astronomical Transient Classification – End-to-End Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end prototype pipeline for astronomical\n",
    "transient classification using SkyPortal-like JSON data.\n",
    "\n",
    "The goals are:\n",
    "- to transform heterogeneous, deeply nested JSON data into structured datasets\n",
    "  suitable for machine learning,\n",
    "- to demonstrate how a local Large Language Model (LLM) can act as a\n",
    "  \"copilot\" to assist astronomers during transient vetting.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Load and inspect the raw JSON data\n",
    "2. Parse and structure the data into ML-ready formats (Parquet)\n",
    "3. Perform sanity checks on the structured datasets\n",
    "4. Use a local LLM (Mistral) to generate copilot-style summaries and suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbce5063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Imports and setup\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from src.parser import load_json\n",
    "from src.dataset import build_dataset, save_parquet as save_sources_parquet\n",
    "from src.lightcurves import build_lightcurve_dataset, save_parquet as save_lc_parquet\n",
    "from src.llm_copilot import (\n",
    "    load_datasets,\n",
    "    run_copilot_for_source,\n",
    "    query_llm,\n",
    ")\n",
    "\n",
    "print(\"Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fdfa0",
   "metadata": {},
   "source": [
    "## 1. Loading the raw JSON data\n",
    "\n",
    "The input JSON file contains a small number of astronomical transients\n",
    "(10–20 objects), but each object is deeply nested and includes rich information\n",
    "such as photometry, spectra, comments, follow-up requests, and derived statistics.\n",
    "\n",
    "As a result, the file is large (tens of thousands of lines), even for a small\n",
    "number of sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31bd412c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sources_sample.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load raw JSON\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[0;32m      5\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/sources_sample.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# adapt path if needed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of sources loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Mes Documents\\Pas cours\\Devoir Stage\\Space\\src\\parser.py:24\u001b[0m, in \u001b[0;36mload_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Load the sources JSON file.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m        List of source objects.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     25\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sources_sample.json'"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Load raw JSON\n",
    "# =========================================\n",
    "\n",
    "json_path = \"data/sources_sample.json\"  # adapt path if needed\n",
    "data = load_json(json_path)\n",
    "\n",
    "print(f\"Number of sources loaded: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e526c4",
   "metadata": {},
   "source": [
    "## 2. Lightweight inspection of the JSON structure\n",
    "\n",
    "Before structuring the data, we briefly inspect the JSON schema to understand\n",
    "which fields are present and to justify our feature selection choices.\n",
    "\n",
    "This step is used for reasoning and documentation, not for exhaustive parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Inspect top-level and TNS keys\n",
    "# =========================================\n",
    "\n",
    "top_level_keys = Counter()\n",
    "tns_keys = Counter()\n",
    "\n",
    "for obj in data:\n",
    "    top_level_keys.update(obj.keys())\n",
    "    if isinstance(obj.get(\"tns_info\"), dict):\n",
    "        tns_keys.update(obj[\"tns_info\"].keys())\n",
    "\n",
    "print(\"Top-level keys:\")\n",
    "top_level_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TNS info keys:\")\n",
    "tns_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eee10b",
   "metadata": {},
   "source": [
    "## 3. Design choices for data structuring\n",
    "\n",
    "From the inspection above, we make the following choices:\n",
    "\n",
    "- **Structured ML features**:\n",
    "  - source metadata (position, score, redshift)\n",
    "  - astrophysical class labels (when available)\n",
    "  - raw photometry time series\n",
    "\n",
    "- **Deferred to LLM reasoning**:\n",
    "  - comments\n",
    "  - follow-up requests\n",
    "  - human annotations and summaries\n",
    "\n",
    "- **Ignored for this prototype**:\n",
    "  - UI-related fields\n",
    "  - deeply nested administrative metadata\n",
    "  - pre-computed photometric statistics (to avoid data leakage)\n",
    "\n",
    "This results in two compact and interpretable datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e261ed0",
   "metadata": {},
   "source": [
    "## 4. Building structured datasets (Part 1)\n",
    "\n",
    "We now convert the raw JSON objects into:\n",
    "- a **source-level dataset** (one row per transient),\n",
    "- a **long-format lightcurve dataset** (one row per photometric measurement).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Build source-level dataset\n",
    "# =========================================\n",
    "\n",
    "sources_df = build_dataset(data)\n",
    "sources_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Build lightcurve dataset\n",
    "# =========================================\n",
    "\n",
    "lightcurves_df = build_lightcurve_dataset(data)\n",
    "lightcurves_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558d161",
   "metadata": {},
   "source": [
    "## 5. Saving datasets to Parquet\n",
    "\n",
    "Parquet is used for compact storage and efficient downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Save datasets\n",
    "# =========================================\n",
    "\n",
    "save_sources_parquet(sources_df, \"data/sources.parquet\")\n",
    "save_lc_parquet(lightcurves_df, \"data/lightcurves.parquet\")\n",
    "\n",
    "print(\"Parquet files written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0144f7b",
   "metadata": {},
   "source": [
    "## 6. Reloading structured datasets\n",
    "\n",
    "We reload the Parquet files to ensure they are self-contained and ready\n",
    "for downstream ML or LLM-based processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de675ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Reload Parquet datasets\n",
    "# =========================================\n",
    "\n",
    "sources_df, lightcurves_df = load_datasets(\n",
    "    \"data/sources.parquet\",\n",
    "    \"data/lightcurves.parquet\",\n",
    ")\n",
    "\n",
    "print(f\"Sources: {len(sources_df)}\")\n",
    "print(f\"Photometry points: {len(lightcurves_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc44a9e",
   "metadata": {},
   "source": [
    "## 7. LLM-based astronomer copilot (Part 2)\n",
    "\n",
    "Instead of training a new classifier, we use a local instruction-tuned LLM\n",
    "(Mistral 7B Instruct) as a reasoning layer on top of structured data.\n",
    "\n",
    "The goal is to:\n",
    "- summarize key information,\n",
    "- assess whether a transient is likely extragalactic,\n",
    "- suggest whether follow-up observations are warranted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Local LLM sanity check\n",
    "# =========================================\n",
    "\n",
    "print(query_llm(\"In one sentence, explain what a supernova is.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cae526",
   "metadata": {},
   "source": [
    "## 8. Running the copilot on a single transient\n",
    "\n",
    "We now run the full copilot pipeline on one example transient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d545d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Run copilot on a single transient\n",
    "# =========================================\n",
    "\n",
    "example_id = sources_df[\"id\"].iloc[0]\n",
    "print(f\"Selected transient: {example_id}\")\n",
    "\n",
    "copilot_output = run_copilot_for_source(\n",
    "    source_id=example_id,\n",
    "    sources=sources_df,\n",
    "    lightcurves=lightcurves_df,\n",
    ")\n",
    "\n",
    "print(\"=== Copilot output ===\")\n",
    "print(copilot_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e71e80",
   "metadata": {},
   "source": [
    "## 9. Running the copilot on multiple transients\n",
    "\n",
    "This demonstrates that the pipeline generalizes beyond a single example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9950c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Run copilot on multiple transients\n",
    "# =========================================\n",
    "\n",
    "for source_id in sources_df[\"id\"].head(3):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Transient ID: {source_id}\")\n",
    "    print(run_copilot_for_source(\n",
    "        source_id=source_id,\n",
    "        sources=sources_df,\n",
    "        lightcurves=lightcurves_df,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c420fb3",
   "metadata": {},
   "source": [
    "## 10. User-driven question\n",
    "\n",
    "Finally, we allow a hypothetical astronomer to ask a specific question\n",
    "about the transient, using the structured context as input to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# User-driven question\n",
    "# =========================================\n",
    "\n",
    "user_question = (\n",
    "    \"Is this transient more likely a supernova or a galactic variable star?\"\n",
    ")\n",
    "\n",
    "combined_prompt = copilot_output + \"\\n\\nUser question:\\n\" + user_question\n",
    "answer = query_llm(combined_prompt)\n",
    "\n",
    "print(\"=== User question answer ===\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b5bf6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "- how to tame large, deeply nested astronomical JSON files,\n",
    "- how to extract a minimal, ML-relevant core dataset,\n",
    "- how to use a local LLM as a reasoning copilot for astronomers.\n",
    "\n",
    "The focus is on clarity, robustness, and design choices rather than\n",
    "model performance or over-engineering.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
